{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convin.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VHd6Owzjv8R"
      },
      "source": [
        "Speech to text conversion module using wav2vec 2.0 developed by Facebook AI.\n",
        "Here we will be using Transformer Architecture Hugging face for the development of Speech Recognition system of Duration more than 15 Minutes.\n",
        "we start with installing transformer and import Wav2Vec2ForCTC, Wav2Vec2Tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZsX1CAEXSfi"
      },
      "source": [
        "!pip install -q transformers\n",
        "import librosa\n",
        "\n",
        "#Importing Pytorch\n",
        "import torch\n",
        "\n",
        "#Importing Wav2Vec\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nNQiz-mc2QK"
      },
      "source": [
        "import IPython.display as display\n",
        "display.Audio(\"/content/sample_data/audio/sampleaudio15plus.wav\", autoplay=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq2OH7mfpcmd"
      },
      "source": [
        "Transformer used in this code is based on attention mechanism: Attention. The attention-mechanism looks at an input sequence and decides at each step which other parts of the sequence are important. It sounds abstract, but let me clarify with an easy example: When reading this text, you always focus on the word you read but at the same time your mind still holds the important keywords of the text in memory in order to provide context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmZH79gnMjUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a532958b-b083-4686-c595-01574b11a582"
      },
      "source": [
        "!pip install pydub"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izcyOxW4kRNU"
      },
      "source": [
        "Inspite of using  T4 GPU and 25GB RAM, we won't be able to process such a large audio file at one time and session would crash even if audio is greater than 2 Minutes with high RAM as well.\n",
        "Hence to deal with hardware limitations for developing long speech recognition system, I will split the audio files of duration 1 Minutes each and then feed them to the pre trained model wav2vec2.0 .\n",
        "\n",
        "we will install pydub for this purpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd3wqYkQMEya"
      },
      "source": [
        "from pydub import AudioSegment\n",
        "import math\n",
        "\n",
        "class SplitWavAudioMubin():\n",
        "    def __init__(self, folder, filename):\n",
        "        self.folder = folder\n",
        "        self.filename = filename\n",
        "        self.filepath = folder + '/' + filename\n",
        "        \n",
        "        self.audio = AudioSegment.from_wav(self.filepath)\n",
        "    \n",
        "    def get_duration(self):\n",
        "        return self.audio.duration_seconds\n",
        "    \n",
        "    def single_split(self, from_min, to_min, split_filename):\n",
        "        t1 = from_min * 60 * 1000\n",
        "        t2 = to_min * 60 * 1000\n",
        "        split_audio = self.audio[t1:t2]\n",
        "        split_audio.export(self.folder + '/' + split_filename, format=\"wav\")\n",
        "        \n",
        "    def multiple_split(self, min_per_split):\n",
        "        total_mins = math.ceil(self.get_duration() / 60)\n",
        "        for i in range(0, total_mins, min_per_split):\n",
        "            split_fn = str(i) + '_' + self.filename\n",
        "            self.single_split(i, i+min_per_split, split_fn)\n",
        "            print(str(i) + ' Done')\n",
        "            if i == total_mins - min_per_split:\n",
        "                print('All splited successfully')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWGdgQXqpImE"
      },
      "source": [
        "About Audio file: Nixon Resignation Audio.\n",
        "\n",
        "\n",
        "Spliting 15 Minutes 22 Seconds Audio file into different chunks of smaller size to deal with the large size audio with pre-trained module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRHs8Es1MMTn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da73283e-7393-425a-ac5b-d08e7cb80712"
      },
      "source": [
        "folder = '/content/sample_data/audio'  \n",
        "file = 'sampleaudio15plus.wav'\n",
        "split_wav = SplitWavAudioMubin(folder, file)\n",
        "split_wav.multiple_split(min_per_split=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Done\n",
            "1 Done\n",
            "2 Done\n",
            "3 Done\n",
            "4 Done\n",
            "5 Done\n",
            "6 Done\n",
            "7 Done\n",
            "8 Done\n",
            "9 Done\n",
            "10 Done\n",
            "11 Done\n",
            "12 Done\n",
            "13 Done\n",
            "14 Done\n",
            "15 Done\n",
            "All splited successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6cuNKOuqDE9"
      },
      "source": [
        "Spliting into 16 Different chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ1u5NKtqDTr"
      },
      "source": [
        "Reading all chunks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eExm7jO0c2Xh"
      },
      "source": [
        "# Loading the audio file\n",
        "audio_split_1, rate_1 = librosa.load(\"/content/sample_data/audio/0_sampleaudio15plus.wav\", sr = 16000)\n",
        "audio_split_2, rate_2 = librosa.load(\"/content/sample_data/audio/1_sampleaudio15plus.wav\", sr = 16000)\n",
        "#audio_split_3, rate_3 = librosa.load(\"/content/sample_data/audio/2_sampleaudio15plus.wav\", sr = 16000)\n",
        "#audio_split_4, rate_4 = librosa.load(\"/content/sample_data/audio/3_sampleaudio15plus.wav\", sr = 16000)\n",
        "#audio_split_5, rate_5 = librosa.load(\"/content/sample_data/audio/4_sampleaudio15plus.wav\", sr = 16000)\n",
        "#audio_split_6, rate_6 = librosa.load(\"/content/sample_data/audio/5_sampleaudio15plus.wav\", sr = 16000)\n",
        "#audio_split_7, rate_7 = librosa.load(\"/content/sample_data/audio/6_sampleaudio15plus.wav\", sr = 16000)\n",
        "#audio_split_8, rate_7 = librosa.load(\"/content/sample_data/audio/7_sampleaudio15plus.wav\", sr = 16000)\n",
        "#audio_split_9, rate_7 = librosa.load(\"/content/sample_data/audio/8_sampleaudio15plus.wav\", sr = 16000)\n",
        "#audio_split_10, rate_7 = librosa.load(\"/content/sample_data/audio/9_sampleaudio15plus.wav\", sr = 16000)\n",
        "#audio_split_11, rate_7 = librosa.load(\"/content/sample_data/audio/10_sampleaudio15plus.wav\", sr = 16000)\n",
        "#audio_split_12, rate_7 = librosa.load(\"/content/sample_data/audio/11_sampleaudio15plus.wav\", sr = 16000)\n",
        "#audio_split_13, rate_7 = librosa.load(\"/content/sample_data/audio/12_sampleaudio15plus.wav\", sr = 16000)\n",
        "#audio_split_14, rate_7 = librosa.load(\"/content/sample_data/audio/13_sampleaudio15plus.wav\", sr = 16000)\n",
        "#audio_split_15, rate_7 = librosa.load(\"/content/sample_data/audio/14_sampleaudio15plus.wav\", sr = 16000)\n",
        "#audio_split_16, rate_7 = librosa.load(\"/content/sample_data/audio/15_sampleaudio15plus.wav\", sr = 16000)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBXbHdeBqXM8"
      },
      "source": [
        "Print out Numpy 1D Array corresponding to each word of a sequence comprising a sentence into a float datatype."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OxZU-dPc2ZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139a73d7-6fce-4b87-998e-59bdf816b8e9"
      },
      "source": [
        "# printing audio \n",
        "print(audio_split_1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.          0.          0.         ... -0.00198364 -0.00476074\n",
            " -0.00595093]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUMol0hFhF-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5495feb4-9838-47e4-fdd4-41f6902f0f1c"
      },
      "source": [
        "print(audio_split_2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.00668335 -0.00613403 -0.006073   ...  0.04379272  0.03759766\n",
            "  0.02746582]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31NAgk_FhGCT",
        "outputId": "ca80d42b-be40-4336-e06b-d7699ac8dfb1"
      },
      "source": [
        "print(audio_split_3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.01119995 -0.0055542  -0.01947021 ...  0.18704224  0.16519165\n",
            "  0.13494873]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhNehKDPhGE3",
        "outputId": "50a61546-3824-4fa3-831f-89e55c00a840"
      },
      "source": [
        "print(audio_split_4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.11380005 0.10412598 0.09487915 ... 0.10473633 0.11230469 0.12936401]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzpnqd9LhGGn",
        "outputId": "1ba99ed8-1e8c-47c1-d731-275b9a7adfa1"
      },
      "source": [
        "print(audio_split_5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.13006592  0.10375977  0.06881714 ... -0.02081299 -0.03250122\n",
            " -0.04101562]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BIbshePhM9w",
        "outputId": "1cd20381-ebe0-4d4c-c1de-2d872a7fe9c3"
      },
      "source": [
        "print(audio_split_6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.04397583 -0.04769897 -0.05847168 ... -0.10162354 -0.09524536\n",
            " -0.0869751 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsNif1ajhNdw",
        "outputId": "6e2d73ce-0732-4a57-9a82-ce4dd9a6993a"
      },
      "source": [
        "print(audio_split_7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.07873535 -0.07296753 -0.07125854 ...  0.08682251  0.08096313\n",
            "  0.0736084 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_zZfkGquC55",
        "outputId": "1c5e3ed3-97a4-4b66-8bb4-f62e4aca93ec"
      },
      "source": [
        "print(audio_split_8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.07064819  0.06732178  0.05935669 ... -0.04177856 -0.03579712\n",
            " -0.02392578]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CEEtLMHuC9n",
        "outputId": "783a7859-de21-4b9f-a745-546097ce86a7"
      },
      "source": [
        "print(audio_split_9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.01089478 -0.00701904 -0.00643921 ... -0.01623535 -0.01571655\n",
            " -0.01431274]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlft5tn4uDBc",
        "outputId": "6ce126ef-ccf0-4d36-9e10-36e36de12eab"
      },
      "source": [
        "print(audio_split_10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.01541138 -0.01663208 -0.01626587 ...  0.0307312  -0.0491333\n",
            "  0.01446533]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUzBiWgCuDET",
        "outputId": "1035bf66-bdde-4865-d596-51e0e911dfc2"
      },
      "source": [
        "print(audio_split_11)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.05410767  0.01318359 -0.0234375  ... -0.00097656 -0.00012207\n",
            " -0.00042725]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNooCr3_uDG3",
        "outputId": "fa57a0dc-725e-46f6-968e-431386dc6b37"
      },
      "source": [
        "print(audio_split_12)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.00128174 0.00128174 ... 0.01818848 0.01989746 0.00387573]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aqmnu4nLuDJK",
        "outputId": "79b65574-f476-44c3-d59e-12b379e02330"
      },
      "source": [
        "print(audio_split_13)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00662231 0.01879883 0.01864624 ... 0.08895874 0.08105469 0.04415894]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSHCtUCvuOOI",
        "outputId": "61e8eddd-786b-4ce6-d0f3-2302efebad27"
      },
      "source": [
        "print(audio_split_14)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.00769043 -0.05606079 -0.07910156 ...  0.00274658  0.00469971\n",
            "  0.00622559]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAlsgu_auOpw",
        "outputId": "0a1dd20a-212f-4452-c037-eb076d893c97"
      },
      "source": [
        "print(audio_split_15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.0043335   0.00390625  0.00701904 ...  0.00259399 -0.00057983\n",
            "  0.00109863]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E56uOOVcuPOK",
        "outputId": "3587af0a-3e5b-4c76-91e3-3eeb6216c407"
      },
      "source": [
        "print(audio_split_16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.003479    0.00137329 -0.00018311 ... -0.00427246 -0.00256348\n",
            "  0.00036621]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omew6JGIc2ba",
        "outputId": "db717c05-0a48-4cd2-bb68-961db094f722"
      },
      "source": [
        "# printing rate\n",
        "print(rate_1)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y4xPvdgq0T1"
      },
      "source": [
        "Importing our Pre-trained modle developed by Facebook AI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00EGzmV-c2dN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf94658-8313-465b-f901-cb42567b9c7f"
      },
      "source": [
        "# Importing Wav2Vec pretrained model\n",
        "\n",
        "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
            "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:419: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
            "  FutureWarning,\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJPTCPi8uQ8P"
      },
      "source": [
        "Now, taking the input values, passing the audio (array) into tokenizer and we want our tensors in PyTorch format instead of Python integers. return_tensors = “pt” which is nothing more than PyTorch format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9X-GuTrc2fL"
      },
      "source": [
        "# Taking an input value for 6 different splits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ilEVFC5h0fq"
      },
      "source": [
        "input_values_1 = tokenizer(audio_split_1, return_tensors = \"pt\").input_values"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGjSRpqYh0i7"
      },
      "source": [
        "input_values_2 = tokenizer(audio_split_2, return_tensors = \"pt\").input_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgJNNzkih0k-"
      },
      "source": [
        "input_values_3 = tokenizer(audio_split_3, return_tensors = \"pt\").input_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQmjUQcJh0na"
      },
      "source": [
        "input_values_4 = tokenizer(audio_split_4, return_tensors = \"pt\").input_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEwayGRkh0qi"
      },
      "source": [
        "input_values_5 = tokenizer(audio_split_5, return_tensors = \"pt\").input_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiNlgKxqiDag"
      },
      "source": [
        "input_values_6 = tokenizer(audio_split_6, return_tensors = \"pt\").input_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-h9tHLyiQvL"
      },
      "source": [
        "input_values_7 = tokenizer(audio_split_7, return_tensors = \"pt\").input_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK0y4QLorsRF"
      },
      "source": [
        "input_values_8 = tokenizer(audio_split_7, return_tensors = \"pt\").input_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrt_BrvErsaM"
      },
      "source": [
        "input_values_9 = tokenizer(audio_split_7, return_tensors = \"pt\").input_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDuG1Qz0rsgk"
      },
      "source": [
        "input_values_10 = tokenizer(audio_split_7, return_tensors = \"pt\").input_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f5FR3JzrsrQ"
      },
      "source": [
        "input_values_11 = tokenizer(audio_split_7, return_tensors = \"pt\").input_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9oxyVH7rs47"
      },
      "source": [
        "input_values_12 = tokenizer(audio_split_7, return_tensors = \"pt\").input_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctlnbbCRr0d7"
      },
      "source": [
        "input_values_13 = tokenizer(audio_split_7, return_tensors = \"pt\").input_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7kxovRdr07B"
      },
      "source": [
        "input_values_14 = tokenizer(audio_split_7, return_tensors = \"pt\").input_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRnV13VQr1Hn"
      },
      "source": [
        "input_values_15 = tokenizer(audio_split_7, return_tensors = \"pt\").input_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAiBReYrr1T3"
      },
      "source": [
        "input_values_16 = tokenizer(audio_split_7, return_tensors = \"pt\").input_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypIOMlhrwBd8"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_1 = model(input_values_1).logits"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMTB4sKcwBjG"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_2 = model(input_values_2).logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62eskrDvSe3d"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_3 = model(input_values_3).logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abuxnn71SlzK"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_4 = model(input_values_4).logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPh_VpW_Sl1L"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_5 = model(input_values_5).logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k_kVggwSl3C"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_6 = model(input_values_6).logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUGk9GomSl4h"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_7 = model(input_values_7).logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpWnk6F2urZ3"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_8 = model(input_values_8).logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5n4IwTeuriL"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_9 = model(input_values_9).logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE-gF3M_uro1"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_10 = model(input_values_10).logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srboPsDxurvC"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_11 = model(input_values_11).logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IhibShGur0O"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_12 = model(input_values_12).logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFfGi6EIur4K"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_13 = model(input_values_13).logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNB3daY-ur8s"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_14 = model(input_values_14).logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogMSkO2TusAx"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_15 = model(input_values_15).logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6uI4PMNusFS"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "logits_16 = model(input_values_16).logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi1PLQijjYpH"
      },
      "source": [
        "# Storing logits (non-normalized prediction values)\n",
        "prediction_1 = torch.argmax(logits_1, dim = -1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDSdxj5PdPi4"
      },
      "source": [
        "# Storing predicted ids\n",
        "prediction_2 = torch.argmax(logits_2, dim = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S28axCCoSwzE"
      },
      "source": [
        "# Storing predicted ids\n",
        "prediction_3 = torch.argmax(logits_3, dim = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrakOMWhSw45"
      },
      "source": [
        "# Storing predicted ids\n",
        "prediction_4 = torch.argmax(logits_4, dim = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOBzqD6rSw60"
      },
      "source": [
        "# Storing predicted ids\n",
        "prediction_5 = torch.argmax(logits_5, dim = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDxzGr8bSw8u"
      },
      "source": [
        "# Storing predicted ids\n",
        "prediction_6 = torch.argmax(logits_6, dim = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mtsLV9SSw-O"
      },
      "source": [
        "# Storing predicted ids\n",
        "prediction_7 = torch.argmax(logits_7, dim = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45ghCIHpvBlT"
      },
      "source": [
        "# Storing predicted ids\n",
        "prediction_8 = torch.argmax(logits_8, dim = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "822n8dkjvBqo"
      },
      "source": [
        "# Storing predicted ids\n",
        "prediction_9 = torch.argmax(logits_9, dim = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFN8AwVWvBwB"
      },
      "source": [
        "# Storing predicted ids\n",
        "prediction_10 = torch.argmax(logits_10, dim = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ9eRq7LvB05"
      },
      "source": [
        "# Storing predicted ids\n",
        "prediction_11 = torch.argmax(logits_11, dim = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdNFEguwvB5X"
      },
      "source": [
        "# Storing predicted ids\n",
        "prediction_12 = torch.argmax(logits_12, dim = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUBl45tAvB8j"
      },
      "source": [
        "# Storing predicted ids\n",
        "prediction_13 = torch.argmax(logits_13, dim = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDNhyARsvCAm"
      },
      "source": [
        "# Storing predicted ids\n",
        "prediction_14 = torch.argmax(logits_14, dim = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIGoRgJ1vCE5"
      },
      "source": [
        "# Storing predicted ids\n",
        "prediction_15 = torch.argmax(logits_15, dim = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iFNo1d5vCJX"
      },
      "source": [
        "# Storing predicted ids\n",
        "prediction_16 = torch.argmax(logits_16, dim = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5BnnMEdo4u-"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_1 = tokenizer.batch_decode(prediction_1)[0]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRYhHSwVdPk-"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_2 = tokenizer.batch_decode(prediction_2)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp70WrgmS5ug"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_3 = tokenizer.batch_decode(prediction_3)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdqKpGvZS5zp"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_4 = tokenizer.batch_decode(prediction_4)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkh7J0uSS53G"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_5 = tokenizer.batch_decode(prediction_5)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nM8S5nkS55F"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_6 = tokenizer.batch_decode(prediction_6)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS9HhGQQTBST"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_7 = tokenizer.batch_decode(prediction_7)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJEhBtcQvXL_"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_8 = tokenizer.batch_decode(prediction_8)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk60q7LKvXPK"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_9 = tokenizer.batch_decode(prediction_9)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cESCOtbzvXXO"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_10 = tokenizer.batch_decode(prediction_10)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP8tUoZ3vXZv"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_11 = tokenizer.batch_decode(prediction_11)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDqvZGZivXck"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_12 = tokenizer.batch_decode(prediction_12)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4WHGwTivXfL"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_13 = tokenizer.batch_decode(prediction_13)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipWcU02yvcJl"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_14 = tokenizer.batch_decode(prediction_14)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A6qQCHHvcQD"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_15 = tokenizer.batch_decode(prediction_15)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSxPVRSmvcVp"
      },
      "source": [
        "# Passing the prediction to the tokenzer decode to get the transcription\n",
        "transcription_16 = tokenizer.batch_decode(prediction_16)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUJGxWSwdWGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11576af-3e49-4d62-9b7d-1c483f7a6d25"
      },
      "source": [
        "# Printing the transcription\n",
        "print(transcription_1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GOOD EVENING THIS IS THE THIRTY SEVENTH TIME I HAVE SPOKEN TO YOU FROM THIS OFFICE WHERE SO MANY DECISIONS HAVE BEEN MADE THAT SHAPED THE HISTORY OF THIS NATION EACH TIME I HAVE DONE SO TO DISCUSS WITH YOU SOME MATTER THAT I BELIEVE EFFECTED THE NATIONAL INTEREST IN ALL THE DECISIONS I HAVE MADE IN MY PUBLIC LIFE I HAVE ALWAYS TRIED TO DO WHAT WAS BEST FOR THE NATION THROUGHOUT THE LONG AND DIFFICULT PERIOD OF WATIGATE I HAVE FELT IT WAS MY DUTY TO PERSEVERE TO MAKE EVERY POSSIBLE EFFORT TO COMPLETE THE TERM OF OFFICE TO WHICH YOU ELECTED ME IN THE BAST FEW DAYS HOWEVER IT HAS BECOME EVIDENT TO ME THAT I NO LONGER HAVE A STRONG ENOUGH POLITICAL BASE IN THE CONGRESS TO JUSTIFY CONTINUING THAT EFFORT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "ZDzjUarhQzMv",
        "outputId": "4be793fc-4832-4c88-9191-bcf2c8988fd7"
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "from google.colab import files\n",
        "\n",
        "#you can save variable into file on colab files\n",
        "\n",
        "joblib.dump(logits,  'logits_1.pkl')   \n",
        " \n",
        "#this will download file to your local downloads\n",
        "\n",
        "files.download('logits_1.pkl')       \n",
        "\n",
        "#reload your saved data.\n",
        "\n",
        "logits = joblib.load('logits_1.pkl') "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_72489052-d905-45cc-83d0-0ef324754436\", \"logits_1.pkl\", 384274)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "LD2Rz58EA9VB",
        "outputId": "82ca886c-3ea1-4663-e690-bb07dfd98145"
      },
      "source": [
        "from itertools import groupby\n",
        "import torch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "import soundfile as sf\n",
        "\n",
        "##############\n",
        "# load model & audio and run audio through model\n",
        "##############\n",
        "#model_name = 'facebook/wav2vec2-large-960h-lv60-self'\n",
        "model_name = \"facebook/wav2vec2-base-960h\"\n",
        "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "audio_filepath = '/content/sample_data/audio/0_sampleaudio15plus.wav'\n",
        "speech, sample_rate = sf.read(audio_filepath)\n",
        "input_values = processor(speech, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
        "\n",
        "logits = model(input_values_1).logits\n",
        "\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "transcription = processor.decode(predicted_ids[0]).lower()\n",
        "\n",
        "##############\n",
        "# this is where the logic starts to get the start and end timestamp for each word\n",
        "##############\n",
        "words = [w for w in transcription.split(' ') if len(w) > 0]\n",
        "predicted_ids = predicted_ids[0].tolist()\n",
        "duration_sec = input_values.shape[1] / sample_rate\n",
        "\n",
        "\n",
        "ids_w_time = [(i / len(predicted_ids) * duration_sec, _id) for i, _id in enumerate(predicted_ids)]\n",
        "# remove entries which are just \"padding\" (i.e. no characers are recognized)\n",
        "ids_w_time = [i for i in ids_w_time if i[1] != processor.tokenizer.pad_token_id]\n",
        "# now split the ids into groups of ids where each group represents a word\n",
        "split_ids_w_time = [list(group) for k, group\n",
        "                    in groupby(ids_w_time, lambda x: x[1] == processor.tokenizer.word_delimiter_token_id)\n",
        "                    if not k]\n",
        "\n",
        "assert len(split_ids_w_time) == len(words)  # make sure that there are the same number of id-groups as words. Otherwise something is wrong\n",
        "# make sure that there are the same number of id-groups as words. Otherwise something is wrong\n",
        "assert len(split_ids_w_time) == len(words), (len(split_ids_w_time), len(words))\n",
        "word_start_times = []\n",
        "word_end_times = []\n",
        "for cur_ids_w_time, cur_word in zip(split_ids_w_time, words):\n",
        "    _times = [_time for _time, _id in cur_ids_w_time]\n",
        "    word_start_times.append(min(_times))\n",
        "    word_end_times.append(max(_times))\n",
        "    \n",
        "#print(\"words are: {}, word_start_times: {}, word_end_times: {}\".format(words, word_start_times, word_end_times))\n",
        "import pandas as pd\n",
        "time_stamp = pd.DataFrame({'word':words, 'word_starting_time': word_start_times, 'word_end_time': word_end_times})\n",
        "time_stamp.head(134)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>word_starting_time</th>\n",
              "      <th>word_end_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>good</td>\n",
              "      <td>3.281094</td>\n",
              "      <td>3.401134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>evening</td>\n",
              "      <td>3.481160</td>\n",
              "      <td>3.701234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this</td>\n",
              "      <td>5.081694</td>\n",
              "      <td>5.201734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>is</td>\n",
              "      <td>5.321774</td>\n",
              "      <td>5.361787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the</td>\n",
              "      <td>5.461821</td>\n",
              "      <td>5.561854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>congress</td>\n",
              "      <td>56.338780</td>\n",
              "      <td>56.778926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>to</td>\n",
              "      <td>57.539180</td>\n",
              "      <td>57.599200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>justify</td>\n",
              "      <td>57.679226</td>\n",
              "      <td>58.199400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>continuing</td>\n",
              "      <td>58.499500</td>\n",
              "      <td>59.119707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>that</td>\n",
              "      <td>59.499833</td>\n",
              "      <td>59.619873</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>134 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           word  word_starting_time  word_end_time\n",
              "0          good            3.281094       3.401134\n",
              "1       evening            3.481160       3.701234\n",
              "2          this            5.081694       5.201734\n",
              "3            is            5.321774       5.361787\n",
              "4           the            5.461821       5.561854\n",
              "..          ...                 ...            ...\n",
              "129    congress           56.338780      56.778926\n",
              "130          to           57.539180      57.599200\n",
              "131     justify           57.679226      58.199400\n",
              "132  continuing           58.499500      59.119707\n",
              "133        that           59.499833      59.619873\n",
              "\n",
              "[134 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "CNdyHZAnHgOL",
        "outputId": "4f2ea5b1-52ab-4c97-ffa3-40def8eba9c5"
      },
      "source": [
        "time_stamp.head(20)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>word_starting_time</th>\n",
              "      <th>word_end_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>good</td>\n",
              "      <td>3.281094</td>\n",
              "      <td>3.401134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>evening</td>\n",
              "      <td>3.481160</td>\n",
              "      <td>3.701234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this</td>\n",
              "      <td>5.081694</td>\n",
              "      <td>5.201734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>is</td>\n",
              "      <td>5.321774</td>\n",
              "      <td>5.361787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the</td>\n",
              "      <td>5.461821</td>\n",
              "      <td>5.561854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>thirty</td>\n",
              "      <td>5.941981</td>\n",
              "      <td>6.162054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>seventh</td>\n",
              "      <td>6.242081</td>\n",
              "      <td>6.522174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>time</td>\n",
              "      <td>6.642214</td>\n",
              "      <td>6.942314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>i</td>\n",
              "      <td>8.042681</td>\n",
              "      <td>8.062688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>have</td>\n",
              "      <td>8.162721</td>\n",
              "      <td>8.282761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>spoken</td>\n",
              "      <td>8.342781</td>\n",
              "      <td>8.642881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>to</td>\n",
              "      <td>8.722908</td>\n",
              "      <td>8.742914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>you</td>\n",
              "      <td>8.802934</td>\n",
              "      <td>8.902968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>from</td>\n",
              "      <td>9.043014</td>\n",
              "      <td>9.123041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>this</td>\n",
              "      <td>9.223074</td>\n",
              "      <td>9.343114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>office</td>\n",
              "      <td>9.503168</td>\n",
              "      <td>9.783261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>where</td>\n",
              "      <td>10.743581</td>\n",
              "      <td>10.903635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>so</td>\n",
              "      <td>11.003668</td>\n",
              "      <td>11.063688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>many</td>\n",
              "      <td>11.183728</td>\n",
              "      <td>11.323775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>decisions</td>\n",
              "      <td>11.403801</td>\n",
              "      <td>11.943981</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         word  word_starting_time  word_end_time\n",
              "0        good            3.281094       3.401134\n",
              "1     evening            3.481160       3.701234\n",
              "2        this            5.081694       5.201734\n",
              "3          is            5.321774       5.361787\n",
              "4         the            5.461821       5.561854\n",
              "5      thirty            5.941981       6.162054\n",
              "6     seventh            6.242081       6.522174\n",
              "7        time            6.642214       6.942314\n",
              "8           i            8.042681       8.062688\n",
              "9        have            8.162721       8.282761\n",
              "10     spoken            8.342781       8.642881\n",
              "11         to            8.722908       8.742914\n",
              "12        you            8.802934       8.902968\n",
              "13       from            9.043014       9.123041\n",
              "14       this            9.223074       9.343114\n",
              "15     office            9.503168       9.783261\n",
              "16      where           10.743581      10.903635\n",
              "17         so           11.003668      11.063688\n",
              "18       many           11.183728      11.323775\n",
              "19  decisions           11.403801      11.943981"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4JDsdtvHop_",
        "outputId": "86cb7dbd-e13a-4f7e-a197-258bef97bac6"
      },
      "source": [
        "print(transcription_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AS LONG AS THERE WAS SUCH A BASE I FELT STRONGLY THAT IT WAS NECESSARY TO SEE THE CONSTITUTIONAL PROCESS THROUGH TO ITS CONCLUSION THAT TO DO OTHERWISE WOULD BE UNFAITHFUL TO THE SPIRIT OF THAT DELIBERATELY DIFFICULT PROCESS AND A DANGEROUSLY DESTABLIZING PRECEDENT FOR THE FUTURE BUT WITH THE DISAPPEARANCE OF THAT BASE I NOW BELIEVE THAT THE CONSTITUTIONAL PURPOSE HAS BEEN SERVED AND THERE IS NO LONGER A NEED FOR THE PROCESS TO BE PROLONGED I WOULD HAVE PREFERRED TO CARRY THROUGH TO THE FINISH WHATEVER THE PERSONAL AGONY IT WOULD HAVE INVOLVED AND MY FAMILY UNANIMOUSLY URGE ME TO DO SO BUT THE INTERESTS OF THE NATION MUST ALWAYS COME BEFORE ANY PERSONAL CONSID\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL1R1hYxwrJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d16c86-a231-4eea-c21b-5e50489e88c9"
      },
      "source": [
        "print(transcription_3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERATIONS FROM THE DISCUSSIONS I HAVE HAD WITH CONGRESSIONAL AND OTHER LEADERS I HAVE CONCLUDED THAT BECAUSE OF THE UATIGATE MATTER I MIGHT NOT HAVE THE SUPPORT OF THE CONGRESS THAT I WOULD CONSIDER NECESSARY TO BACK THE VERY DIFFICULT DECISIONS AND CARRY OUT THE DUTIES OF THIS OFFICE IN THE WAY THE INTERESTS OF THE NATION A REQUIRE I HAVE NEVER BEEN A QUITTER TO LEAVE OFFICE BEFORE MY TERM IS COMPLETED IS ABHORRENT TO EVERY INSTINCT IN MY BODY BUT AS PRESIDENT I MUST PUT THE INTERESTS OF AMERICA FIRST AMERICA NEEDS A FULL TIME PRESIDENT AND A FULL TIME CONGRESS PARTICULARLY AT THIS TIME WITH PROBLEMS WE FACE AT HOME AND ABROAD TO CONTINUE TO FIGHT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLsiUEeeTq6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df992f20-3340-4330-b0a2-7e819b7d67b8"
      },
      "source": [
        "print(transcription_4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OUGH THE MONTHS AHEAD FOR MY PERSONAL VINDICATION WOULD ALMOST TOTALLY ABSORB THE TIME AND ATTENTION OF BOTH THE PRESIDENT AND THE CONGRESS IN A PERIOD WHEN OUR ENTIRE FOCUS SHOULD BE ON THE GREAT ISSUES OF PEACE ABROAD AND PROSPERITY WITHOUT INFLATION AT HOME THEREFORE I SHALL RESIGN THE PRESIDENCY EFFECTIVE AT NOON TO MORROW VICE PRESIDENT FOR WILL BE SWORN IN THIS PRESIDENT AT THAT HOUR IN THIS OFFICE AS I RECALL THE HIGH HOPES FOR AMERICA WITH WHICH WE BEGAN THIS SECOND TURN I FEEL A GREAT SADNESS THAT I WILL NOT BE HERE IN THIS OFFICE WORKING ON YOUR BEHALF TO ACHIEVE THOSE HOPES IN THE NEXT TWO AND A HALF YEARS BUT IN TURNING OVER DIRECTION OF THE GOVERNMENT TO VICE PRESIDENT FORD I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MeSu4k7Tq72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e777dd8-39a8-41d2-bce1-6ad772bf4337"
      },
      "source": [
        "print(transcription_5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNOW AS I TOLD THE NATION WHEN I NOMINATE HIM FOR THAT OFFICE TEN MONTHS AGO THAT THE LEADERSHIP OF AMERICA WILL BE IN GOOD HANDS IN PASSING THIS OFFICE TO THE VICE PRESIDENT I ALSO DO SO WITH A PROFOUND SENSE OF THE WEIGHT OF RESPONSIBILITY THAT WILL FALL ON HIS SHOULDERS TO MORROW AND THEREFORE OF THE UNDERSTANDING THE PATIENCE THE CO OPERATION HE WILL NEED FROM ALL AMERICANS AS HE ASSUMES THAT RESPONSIBILITY HE WILL DESERVE THE HELP AND THE SUPPORT OF ALL OF US AS WE LOOK TO THE FUTURE THE FIRST ESSENTIAL IS TO BEGIN HEALING THE WOUNDS OF THIS NATION TO PUT THE BITTERNESS AND DIVISIONS OF THE RECENT PAST BEHIND US AND TO REDISCU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJeh1DJxTq9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c3f4c8-1d22-4f50-e4b9-11d6d121fbf7"
      },
      "source": [
        "print(transcription_6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VER THOSE SHARED IDEALS THAT LIETH THE HEART OF OUR STRENGTH AND UNITY AS A GRAVE AND AS A FREE PEOPLE BY TAKING THIS ACTION I HOPE THAT I WILL HAVE HASTENED THE START OF THAT PROCESS OF HEALI WHICH IS SO DESPERATELY NEEDED IN AMERICA I REGRET DEEPLY ANY INJURIES THAT MAY HAVE BEEN DONE IN THE COURSE OF THE EVENTS THAT LED TO THIS DECISION I WOULD SAY ONLY THAT IF SOME OF MY JUDGMENTS WERE WRONG AND SOME WERE WRONG THEY WERE MADE IN WHAT I BELIEVED AT THE TIME TO BE THE BEST INTEREST OF THE NATION TO THOSE WHO HAVE STOOD WITH ME DURING THESE PAST DIFFICULT MONTHS TO MY FAMILY MY FRIENDS THE MANY OTHERS WHO JOINED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FJt8HeFTvBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e6ba63-9842-44b4-d6b0-1f72a0652284"
      },
      "source": [
        "print(transcription_7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E20DXNUwvujt",
        "outputId": "2b795081-bffd-48e0-c564-5faa0aa3c598"
      },
      "source": [
        "print(transcription_8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK6A25IAvunr",
        "outputId": "e7bd3323-dfd2-42db-d329-6d5003acbbb2"
      },
      "source": [
        "print(transcription_9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5s14ciKvutj",
        "outputId": "970a2766-1c4b-4915-9aa9-5fa219250150"
      },
      "source": [
        "print(transcription_10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0Qd_0rWvuzd",
        "outputId": "7f90a760-1e74-4c10-988a-d09ef20111fc"
      },
      "source": [
        "print(transcription_11)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqdPTbvOvu4b",
        "outputId": "c30731f4-ea01-4930-dce7-d010c137fa1c"
      },
      "source": [
        "print(transcription_12)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahswlBb7vu60",
        "outputId": "a48b74f6-90d3-408e-ac35-6ab711a7a5a7"
      },
      "source": [
        "print(transcription_13)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Grpu8mm-vu-L",
        "outputId": "782d66c0-4f2f-45f6-8483-430bdc8c1911"
      },
      "source": [
        "print(transcription_14)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYboRw_IvvA-",
        "outputId": "c1b14fc8-b52f-4208-d293-b97eaacf1032"
      },
      "source": [
        "print(transcription_15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMu5YbNjvvEB",
        "outputId": "485b1c6b-8669-409b-a219-d46003a28c3e"
      },
      "source": [
        "print(transcription_16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt_0XvEmTwAr"
      },
      "source": [
        "full_text = transcription_1 + transcription_2 + transcription_3 + transcription_4 + transcription_5 + transcription_6 + transcription_7 + transcription_8 + transcription_9 + transcription_10 + transcription_11 + transcription_12 + transcription_13 + transcription_14 + transcription_15 + transcription_16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBmEizySjCl-",
        "outputId": "0ec7883f-feac-4ec8-d16c-8e7e0c9f68f2"
      },
      "source": [
        "print(full_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GOOD EVENING THIS IS THE THIRTY SEVENTH TIME I HAVE SPOKEN TO YOU FROM THIS OFFICE WHERE SO MANY DECISIONS HAVE BEEN MADE THAT SHAPED THE HISTORY OF THIS NATION EACH TIME I HAVE DONE SO TO DISCUSS WITH YOU SOME MATTER THAT I BELIEVE EFFECTED THE NATIONAL INTEREST IN ALL THE DECISIONS I HAVE MADE IN MY PUBLIC LIFE I HAVE ALWAYS TRIED TO DO WHAT WAS BEST FOR THE NATION THROUGHOUT THE LONG AND DIFFICULT PERIOD OF WATIGATE I HAVE FELT IT WAS MY DUTY TO PERSEVERE TO MAKE EVERY POSSIBLE EFFORT TO COMPLETE THE TERM OF OFFICE TO WHICH YOU ELECTED ME IN THE BAST FEW DAYS HOWEVER IT HAS BECOME EVIDENT TO ME THAT I NO LONGER HAVE A STRONG ENOUGH POLITICAL BASE IN THE CONGRESS TO JUSTIFY CONTINUING THAT EFFORTAS LONG AS THERE WAS SUCH A BASE I FELT STRONGLY THAT IT WAS NECESSARY TO SEE THE CONSTITUTIONAL PROCESS THROUGH TO ITS CONCLUSION THAT TO DO OTHERWISE WOULD BE UNFAITHFUL TO THE SPIRIT OF THAT DELIBERATELY DIFFICULT PROCESS AND A DANGEROUSLY DESTABLIZING PRECEDENT FOR THE FUTURE BUT WITH THE DISAPPEARANCE OF THAT BASE I NOW BELIEVE THAT THE CONSTITUTIONAL PURPOSE HAS BEEN SERVED AND THERE IS NO LONGER A NEED FOR THE PROCESS TO BE PROLONGED I WOULD HAVE PREFERRED TO CARRY THROUGH TO THE FINISH WHATEVER THE PERSONAL AGONY IT WOULD HAVE INVOLVED AND MY FAMILY UNANIMOUSLY URGE ME TO DO SO BUT THE INTERESTS OF THE NATION MUST ALWAYS COME BEFORE ANY PERSONAL CONSIDERATIONS FROM THE DISCUSSIONS I HAVE HAD WITH CONGRESSIONAL AND OTHER LEADERS I HAVE CONCLUDED THAT BECAUSE OF THE UATIGATE MATTER I MIGHT NOT HAVE THE SUPPORT OF THE CONGRESS THAT I WOULD CONSIDER NECESSARY TO BACK THE VERY DIFFICULT DECISIONS AND CARRY OUT THE DUTIES OF THIS OFFICE IN THE WAY THE INTERESTS OF THE NATION A REQUIRE I HAVE NEVER BEEN A QUITTER TO LEAVE OFFICE BEFORE MY TERM IS COMPLETED IS ABHORRENT TO EVERY INSTINCT IN MY BODY BUT AS PRESIDENT I MUST PUT THE INTERESTS OF AMERICA FIRST AMERICA NEEDS A FULL TIME PRESIDENT AND A FULL TIME CONGRESS PARTICULARLY AT THIS TIME WITH PROBLEMS WE FACE AT HOME AND ABROAD TO CONTINUE TO FIGHTOUGH THE MONTHS AHEAD FOR MY PERSONAL VINDICATION WOULD ALMOST TOTALLY ABSORB THE TIME AND ATTENTION OF BOTH THE PRESIDENT AND THE CONGRESS IN A PERIOD WHEN OUR ENTIRE FOCUS SHOULD BE ON THE GREAT ISSUES OF PEACE ABROAD AND PROSPERITY WITHOUT INFLATION AT HOME THEREFORE I SHALL RESIGN THE PRESIDENCY EFFECTIVE AT NOON TO MORROW VICE PRESIDENT FOR WILL BE SWORN IN THIS PRESIDENT AT THAT HOUR IN THIS OFFICE AS I RECALL THE HIGH HOPES FOR AMERICA WITH WHICH WE BEGAN THIS SECOND TURN I FEEL A GREAT SADNESS THAT I WILL NOT BE HERE IN THIS OFFICE WORKING ON YOUR BEHALF TO ACHIEVE THOSE HOPES IN THE NEXT TWO AND A HALF YEARS BUT IN TURNING OVER DIRECTION OF THE GOVERNMENT TO VICE PRESIDENT FORD IKNOW AS I TOLD THE NATION WHEN I NOMINATE HIM FOR THAT OFFICE TEN MONTHS AGO THAT THE LEADERSHIP OF AMERICA WILL BE IN GOOD HANDS IN PASSING THIS OFFICE TO THE VICE PRESIDENT I ALSO DO SO WITH A PROFOUND SENSE OF THE WEIGHT OF RESPONSIBILITY THAT WILL FALL ON HIS SHOULDERS TO MORROW AND THEREFORE OF THE UNDERSTANDING THE PATIENCE THE CO OPERATION HE WILL NEED FROM ALL AMERICANS AS HE ASSUMES THAT RESPONSIBILITY HE WILL DESERVE THE HELP AND THE SUPPORT OF ALL OF US AS WE LOOK TO THE FUTURE THE FIRST ESSENTIAL IS TO BEGIN HEALING THE WOUNDS OF THIS NATION TO PUT THE BITTERNESS AND DIVISIONS OF THE RECENT PAST BEHIND US AND TO REDISCUVER THOSE SHARED IDEALS THAT LIETH THE HEART OF OUR STRENGTH AND UNITY AS A GRAVE AND AS A FREE PEOPLE BY TAKING THIS ACTION I HOPE THAT I WILL HAVE HASTENED THE START OF THAT PROCESS OF HEALI WHICH IS SO DESPERATELY NEEDED IN AMERICA I REGRET DEEPLY ANY INJURIES THAT MAY HAVE BEEN DONE IN THE COURSE OF THE EVENTS THAT LED TO THIS DECISION I WOULD SAY ONLY THAT IF SOME OF MY JUDGMENTS WERE WRONG AND SOME WERE WRONG THEY WERE MADE IN WHAT I BELIEVED AT THE TIME TO BE THE BEST INTEREST OF THE NATION TO THOSE WHO HAVE STOOD WITH ME DURING THESE PAST DIFFICULT MONTHS TO MY FAMILY MY FRIENDS THE MANY OTHERS WHO JOINEDIN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE FIN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE FIN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE FIN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE FIN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE FIN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE FIN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE FIN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE FIN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE FIN SUPPORTING MY CAUSE BECAUSE THEY BELIEVED IT WAS RIGHT I WILL BE ETERNALLY GRATEFUL FOR YOUR SUPPORT AND WITHOSE WHO HAVE NOT FELT ABLE TO GIVE ME YOUR SUPPORT LET ME SAY I LEAVE WITH NO BITTERNESS TOWARD THOSE WHILL OPPOSE ME BECAUSE ALL OF US IN THE FINAL ANALYSIS HAVE BEEN CONCERNED WITH THE GOOD OF THE COUNTRY HOWEVER OUR JUDGMENTS MIGHT DIFFER SO LET US ALL NOW JOIN TOGETHER AND AFFIRMING THAT COMMON COMMITMENT AND IN HELPING OUR NEW PRESIDENT SUCCEED FOR THE BENEFIT OF ALL MOURRS I SHALL LEAVE THIS OFFICE WITH REGRET AT NOT COMPLETING MY TERM BUT WITH GRATITUDE F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7ZLvBbTz6Ck"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}